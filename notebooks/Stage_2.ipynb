{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiGD3stUYm0C",
        "outputId": "dda75bb8-cd55-47a0-d072-d7149fcfea67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1h74ECRl7Aqb7zZk6WJch-xmqhW6mzH3-\n",
        "# !gdown 1x9BRcMcdobE23K2vyLFpiqVZhHXtFS2S\n",
        "# !gdown 1KHJpzofsASSa0DfBZINmqsFQ1FgRvj_J\n",
        "# !gdown 1SbKL3cPZTw8jjq-L_O9mzZHcMCBPEEdR\n",
        "!gdown 1zbhaZYGWg5BdjpuofGs01ZXXQ9mQzJKy\n",
        "!gdown 1DbBDg0tdx1GSRJ0WHgBQTsRwIPyGYCMP\n",
        "!gdown 1x44fAZLzCFxWaPF7R0MX9V_V744u7Std\n",
        "\n",
        "!gdown 1SGYIwG_yCoUegu9lcm8YmSqWfcZoU-X7   # val false 1\n",
        "!gdown 1t7BhSyt362WqHJLHIIPao-mhQRYAIWMm   # val false 2\n",
        "!gdown 1OsfHDqipyifRCTgmH8p-VXpk3KoZzWSn   # val false 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdYAM9VHZd5u",
        "outputId": "0e05680a-2ef3-498e-e0d4-c4b0a6dcb7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zbhaZYGWg5BdjpuofGs01ZXXQ9mQzJKy\n",
            "To: /content/test_data.txt\n",
            "100% 817k/817k [00:00<00:00, 116MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DbBDg0tdx1GSRJ0WHgBQTsRwIPyGYCMP\n",
            "To: /content/scores_dict.pkl\n",
            "100% 247M/247M [00:03<00:00, 73.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x44fAZLzCFxWaPF7R0MX9V_V744u7Std\n",
            "To: /content/train_val_folds.pkl\n",
            " 91% 2.75G/3.03G [00:43<00:02, 92.7MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "8z7M3K66kv-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install loguru"
      ],
      "metadata": {
        "id": "F841chvok5mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from datasets import DatasetDict, Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from loguru import logger\n",
        "import pickle\n",
        "\n",
        "sys.path.append('')\n",
        "\n",
        "def load_model_from_checkpoint(path_to_checkpoint):\n",
        "    ''' Helper function, to load the model from a checkpoint.\n",
        "    takes as input a path to the checkpoint (from the \"experiment-[...]\" )\n",
        "     '''\n",
        "    full_path_to_model_checkpoint = experiment_path + path_to_checkpoint\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(full_path_to_model_checkpoint, num_labels=2, local_files_only=False, ignore_mismatched_sizes=True)\n",
        "    print(f\"Loaded model from: {full_path_to_model_checkpoint}\")\n",
        "    return model\n",
        "\n",
        "def numpy_softmax(model_preds):\n",
        "    '''Converts the raw predictions from a HuggingFace model into clean logits.'''\n",
        "    max = np.max(model_preds, axis=1, keepdims=True)\n",
        "    e_x = np.exp(model_preds-max)\n",
        "    sum = np.sum(e_x, axis=1, keepdims=True)\n",
        "    out = e_x / sum\n",
        "    return out\n",
        "\n",
        "def load_tweets(file_path):\n",
        "    tweets = list()\n",
        "    with open(file_path, 'r', encoding='utf-8') as preprocessed_tweets:\n",
        "        for tweet in preprocessed_tweets:\n",
        "            tweets.append(tweet.rstrip('\\n'))\n",
        "    return tweets\n",
        "\n",
        "def preprocess_function(examples, tok_max_length):\n",
        "    return tokenizer(examples[\"tweet\"], truncation=True, max_length=tok_max_length, padding=True)\n",
        "\n",
        "\n",
        "def save_dictionary_as_pickle(dictionary, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(dictionary, file)\n",
        "\n",
        "def load_pickle_as_dictionary(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        dictionary = pickle.load(file)\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "def get_mispredicted_samples(X, y_pred, y_true):\n",
        "    mispredicted_X = []\n",
        "    mispredicted_Y = []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        if y_pred[i] != y_true[i]:\n",
        "            mispredicted_X.append(X[i])\n",
        "            mispredicted_Y.append(y_true[i])\n",
        "\n",
        "    return mispredicted_X, mispredicted_Y\n",
        "\n",
        "def sort_by_difficulty(dataset, dict_path):\n",
        "    subset_X = dataset['tweet']\n",
        "    subset_y = dataset['label']\n",
        "    scores_dict = load_pickle_as_dictionary(dict_path)\n",
        "\n",
        "    new_dict = {}\n",
        "    for i, x in enumerate(subset_X):\n",
        "      new_dict[i] = {'x': x, 'label': subset_y[i], 'score': scores_dict[x]['score']}\n",
        "\n",
        "    new_sorted_dict = dict(sorted(new_dict.items(), key=lambda item: item[1]['score'], reverse=False))\n",
        "    sorted_X = [item[1]['x'] for item in new_sorted_dict.items()]\n",
        "    sorted_y = [item[1]['label'] for item in new_sorted_dict.items()]\n",
        "\n",
        "    new_data = {\"tweet\": sorted_X, \"label\": sorted_y}\n",
        "    new_dataset = Dataset.from_dict(new_data)\n",
        "    new_tokenized_dataset = new_dataset.map(lambda examples: preprocess_function(examples, tok_max_length), batched=True)\n",
        "\n",
        "    return new_tokenized_dataset\n",
        "\n",
        "def interleave(dataset):\n",
        "    X_pos_train, y_pos_train = list(zip(*filter(lambda t: t[1] == 1, zip(dataset['tweet'], dataset['label']))))\n",
        "    X_neg_train, y_neg_train = list(zip(*filter(lambda t: t[1] == 0, zip(dataset['tweet'], dataset['label']))))\n",
        "\n",
        "    a, b = [X_pos_train, y_pos_train], [X_neg_train, y_neg_train]\n",
        "    n = len(a[0]) + len(b[0])\n",
        "    results = []\n",
        "    for j in range(2):\n",
        "      true_ratio = len(a[j]) / n\n",
        "      c = []\n",
        "      a_count, b_count = 0, 0\n",
        "      running_ratio = 0\n",
        "      for i in range(n):\n",
        "          if running_ratio < true_ratio:\n",
        "              c.append(a[j][a_count])\n",
        "              a_count += 1\n",
        "          else:\n",
        "              c.append(b[j][b_count])\n",
        "              b_count += 1\n",
        "          running_ratio = a_count / (a_count + b_count)\n",
        "      results.append(c)\n",
        "\n",
        "    new_data = {\"tweet\": results[0], \"label\": results[1]}\n",
        "    new_dataset = Dataset.from_dict(new_data)\n",
        "    new_tokenized_dataset = new_dataset.map(lambda examples: preprocess_function(examples, tok_max_length), batched=True)\n",
        "\n",
        "    return new_tokenized_dataset\n",
        "\n",
        "def concat_datasets (Dataset1, Dataset2):\n",
        "\n",
        "    # Convert dataset1 to a pandas DataFrame\n",
        "    df1 = Dataset1.to_pandas()\n",
        "    # Convert dataset2 to a pandas DataFrame\n",
        "    df2 = Dataset2.to_pandas()\n",
        "    # Concatenate the DataFrames\n",
        "    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n",
        "    # Convert the concatenated DataFrame back to a Dataset\n",
        "    concatenated_dataset = Dataset.from_pandas(concatenated_df)\n",
        "    return concatenated_dataset\n",
        "\n",
        "# Set default values for the variables\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "batch_size = 32\n",
        "seed = 12222\n",
        "fp16 = True\n",
        "out = \"./logging\"\n",
        "epochs = 2\n",
        "lr = 1e-4\n",
        "wd = 0.005\n",
        "tok_max_length = 128\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "time_run = time.time()\n",
        "\n",
        "project_path = \"./\"\n",
        "experiment_path = \"./\" + \"Experiments/\"\n",
        "\n",
        "experiment_date_for_folder_name = \"experiment-\" + model_name + \"_\" + \"default\"\n",
        "\n",
        "experiments_results_path = experiment_path + experiment_date_for_folder_name\n",
        "os.makedirs(experiments_results_path, exist_ok=True)\n",
        "checkpoints_path = experiments_results_path + \"/checkpoints/\"\n",
        "print(\"The project path is: \", project_path)\n",
        "print(\"The experiment path is: \", experiment_path)\n",
        "print(\"The model checkpoints will be saved at: \", checkpoints_path, \"\\n\")\n",
        "\n",
        "# for the submission\n",
        "test_results_path = experiments_results_path + \"/test_results/\"\n",
        "os.makedirs(test_results_path, exist_ok=True)\n",
        "\n",
        "# for validation results\n",
        "val_results_path = experiments_results_path + \"/val_results/\"\n",
        "os.makedirs(val_results_path, exist_ok=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device {device}')\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "test_tweets = load_tweets('/content/test_data.txt')\n",
        "\n",
        "with open('/content/train_val_folds.pkl', 'rb') as file:\n",
        "    train_val_folds = pickle.load(file)\n",
        "\n",
        "with open('/content/val_false_tokenized_dataset1.pkl', 'rb') as file:\n",
        "    val_false_dataset1 = pickle.load(file)\n",
        "\n",
        "with open('/content/val_false_tokenized_dataset2.pkl', 'rb') as file:\n",
        "    val_false_dataset2 = pickle.load(file)\n",
        "\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "logging_steps = 4000\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=out,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    save_total_limit=2,\n",
        "    seed=seed,\n",
        "    weight_decay=wd,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    disable_tqdm=False,\n",
        "    fp16=fp16,\n",
        "    logging_steps=logging_steps,\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        "    warmup_steps=500\n",
        ")\n",
        "\n",
        "dictionary_path = '/content/scores_dict.pkl'\n",
        "\n",
        "train_dataset = concat_datasets(train_val_folds[2][0], val_false_dataset1)\n",
        "train_dataset = concat_datasets(train_dataset, val_false_dataset2)\n",
        "train_dataset_sorted = sort_by_difficulty(train_dataset, dictionary_path)\n",
        "train_interleave_dataset = interleave(train_dataset_sorted)\n",
        "\n",
        "\n",
        "evaluation_dataset_sorted = sort_by_difficulty(train_val_folds[2][1], dictionary_path)\n",
        "evaluation_interleave_dataset = interleave(evaluation_dataset_sorted)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_interleave_dataset,\n",
        "    eval_dataset=evaluation_interleave_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "logger.info(f\"Started training\")\n",
        "trainer.train()\n",
        "logger.info(f\"Ended training\")\n",
        "\n",
        "data_test = pd.DataFrame({'tweet': test_tweets})\n",
        "test_dataset = Dataset.from_dict(data_test)\n",
        "test_dataset = test_dataset.map(lambda examples: preprocess_function(examples, tok_max_length), batched=True)\n",
        "\n",
        "results_test = trainer.predict(test_dataset)\n",
        "y_preds_test = np.argmax(results_test.predictions, axis=1)\n",
        "\n",
        "results_val = trainer.predict(train_val_folds[2][1])\n",
        "y_preds_val = np.argmax(results_val.predictions, axis=1)\n",
        "\n",
        "y_preds_test = [-1 if test == 0 else 1 for test in y_preds_test]\n",
        "\n",
        "X_val_false, y_val_false = get_mispredicted_samples(train_val_folds[2][1]['tweet'],y_preds_val,train_val_folds[2][1]['label'])\n",
        "\n",
        "val_data_false = {\"tweet\": X_val_false, \"label\": y_val_false}\n",
        "# Convert the dictionary to a Dataset object\n",
        "val_false_dataset = Dataset.from_dict(val_data_false)\n",
        "\n",
        "#Tokenization using map\n",
        "val_false_tokenized_dataset = val_false_dataset.map(lambda examples: preprocess_function(examples, tok_max_length), batched=True)\n",
        "\n",
        "# # Save val_false locally\n",
        "# with open('/content/val_false_tokenized_dataset1.pkl', 'wb') as file:\n",
        "#     pickle.dump(val_false_tokenized_dataset, file)\n",
        "\n",
        "# # Save val_false on Google Drive\n",
        "# with open('/content/drive/MyDrive/val_false_tokenized_dataset1.pkl', 'wb') as file:\n",
        "#     pickle.dump(val_false_tokenized_dataset, file)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(y_preds_test, columns=[\"Prediction\"])\n",
        "df.index.name = \"Id\"\n",
        "df.index += 1\n",
        "df.to_csv(test_results_path + f\"test_data6.csv\")\n",
        "\n",
        "logits_val = numpy_softmax(results_val.predictions)\n",
        "logits_test = numpy_softmax(results_test.predictions)\n",
        "\n",
        "os.makedirs(test_results_path + model_name + \"-\" + 'logits_test6.txt', exist_ok=True)\n",
        "np.savetxt(test_results_path + f\"logits_test6.txt\", logits_test, delimiter=\",\", header=\"negative,positive\", comments=\"\")\n",
        "\n",
        "os.makedirs(val_results_path + model_name + \"-\" + 'logits_val6.txt', exist_ok=True)\n",
        "np.savetxt(val_results_path + f\"logits_val6.txt\", logits_val, delimiter=\",\", header=\"negative,positive\", comments=\"\")\n",
        "\n",
        "time_total = time.time() - time_run\n",
        "print(f\"The program took {str(time_total/60/60)[:6]} Hours or {str(time_total/60)[:6]} minutes to run.\")\n"
      ],
      "metadata": {
        "id": "efSCJji-Ez11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Experiments.zip /content/Experiments\n",
        "!zip -r /content/logs.zip /content/logs\n",
        "!zip -r /content/logging.zip /content/logging"
      ],
      "metadata": {
        "id": "k8BtxsSuUa8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Experiments.zip\")\n",
        "files.download(\"/content/logs.zip\")\n",
        "files.download(\"/content/logging.zip\")"
      ],
      "metadata": {
        "id": "5P5Nw5KHSI59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}