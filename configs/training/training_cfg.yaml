SYSTEM:
  SEED_VALUE: 12222
  FP16: True

EXPERIMENT:
  NAME: 'Bert-XGB'

IO:
  PROJECT_PATH: './'
  EXPERIMENT_PATH: './Experiments/'
  DICTIONARY_PATH: './scores_dict.pkl'
  LOG_PATH: "./logging"
  PREPROCESSED_POS_DATA_PATH: './processed_pos_tweets_non_transformer.txt'
  PREPROCESSED_NEG_DATA_PATH: './processed_neg_tweets_non_transformer.txt'
  PREPROCESSED_TEST_DATA_PATH: './processed_test_tweets_non_transformer.txt'
  FT_TRAIN_DATA_FILE_PATH: './train_data.txt'
  FT_VAL_DATA_FILE_PATH: './val_data.txt'
  FT_TEST_DATA_FILE_PATH: './test_data.txt'
  TEST_PREDICTIONS_FILE_PATH: './test_data_fasttext.csv'
  GLOVE_EMBEDDING_FILE_PATH: './glove.twitter.27B.200d.txt'

TF_IDF:
  TEST_VAL_SPLIT_RATIO: 0.05
  MODEL: 'non-linear' # or 'non-linear'
  CW: 'balanced'
  PENALTY: 'l2'
  NUM_OF_ESTIMATORS: 200
  MAX_DEPTH: 11
  VERBOSE: 4

FASTTEXT:
  TEST_VAL_SPLIT_RATIO: 0.05
  LR: 0.01
  DIMENSION: 150
  EPOCH: 20

GLOVE:
  TEST_VAL_SPLIT_RATIO: 0.05
  NUM_OF_ESTIMATORS: 850
  SUBSAMPLE: 0.8
  TREE_METHOD: 'gpu_hist'
  OBJECTIVE: 'binary:logistic'
  USE_LABEL_ENCODER: False

W2V:
  TEST_VAL_SPLIT_RATIO: 0.05
  MIN_COUNT: 5
  SAMPLE: 5e-5
  WINDOW: 3
  ALPHA: 0.035
  MIN_ALPHA: 0.00075
  NEGATIVE: 5
  NUM_OF_WORKERS: 4
  VECTOR_SIZE: 250
  EPOCHS: 25
  NUM_OF_ESTIMATORS: 1250
  TREE_METHOD: 'gpu_hist'
  OBJECTIVE: 'binary:logistic'

LSTM:
  TEST_VAL_SPLIT_RATIO: 0.05
  EMBEDDING_DIMENSION: 500
  MAXIMUM_FEATURES: 15000
  MAXIMUM_LENGTH: 100
  UNITS: 200
  DROPOUT: 0.3
  RECURRENT_DROPOUT: 0.25
  SPATIAL_DROPOUT: 0.35
  DENSE_LAYER_SIZE: 2
  DENSE_LAYER_ACTIVATION: 'softmax'
  MODEL_OPTIMIZER: 'adam'
  MODEL_LOSS: 'categorical_crossentropy'
  BATCH_SIZE: 128
  EPOCHS: 25
  STEPS_PER_EPOCH: 300
  VERBOSE: 1

BERTXGB:
  TEST_VAL_SPLIT_RATIO: 0.05
  DEVICE: 'cpu'
  MAXIMUM_LENGTH: 128
  NUM_OF_ESTIMATORS: 850
  TREE_METHOD: 'gpu_hist'
  OBJECTIVE: 'binary:logistic'
  USE_LABEL_ENCODER: False

NHP:
  MODEL_NAME: "bert-base-uncased"
  BATCH_SIZE: 32
  EPOCHS: 3
  LR: 1e-4
  WD: 0.005
  TOK_MAX_LENGTH: 128